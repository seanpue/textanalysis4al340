{
 "metadata": {
  "name": "",
  "signature": "sha256:c6e1cd737c14ce56a48c4de3f284ee443cfddb97a24b8ab81ac588d995c31abe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Textual Analysis in Python (for DHers, etc).\n",
      "# Part Two: Natural Language Toolkit (NLTK)\n",
      "\n",
      "Author: [A. Sean Pue](http://seanpue.com)\n",
      "\n",
      "A lesson for [AL340 Digital Humanities Seminar](http://seanpue.github.io/al340) (Spring 2015)\n",
      "\n",
      "--> Double Click here to start<--"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Welcome!\n",
      "This is an IPython Notebook. It contains numerous cells that can be of different types (Markdown, Code, Headings). This is lesson two. You may need to [install the Natural Language Toolkit](http://www.nltk.org/install.html) to begin.\n",
      "\n",
      "#### Your turn\n",
      "\n",
      "Select the cell above this one by double clicking it with your mouse.\n",
      "\n",
      "You can see that it contains text in a format called Markdown. To execute the cell,\n",
      "press `shift+enter.` To complete this tutorial (which is meant for a classroom), execute the cells and follow the instructions.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#The next line imports the nltk\n",
      "import nltk\n",
      "# next line makes graphics appear in the browser rather than separate window\n",
      "%matplotlib inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Let's build a corpus."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we will work with this file: http://www.gutenberg.org/cache/epub/11/pg11.txt"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "First, let's download it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "url = 'http://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
      "\n",
      "# get text\n",
      "t = urllib2.urlopen(url).read().decode('utf8')\n",
      "assert(t)\n",
      "len(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Next, let's remove the header and footer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_starts= t.index('CHAPTER I') # find where chapter starts\n",
      "text_ends = t.index('THE END')\n",
      "t = t[text_starts:text_ends]\n",
      "len(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Let's split the text into different chapters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re # imports regular expression module (which allows fancy text searching)\n",
      "chapters = re.findall('CHAPTER .+?(?=CHAPTER|$)',t,re.DOTALL) # This is a regular expression that splits out chapters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here is a sample of chapter 2 (or chapter[1])\n",
      "chapters[1][0:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Now let's write the chapters to separate text files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for chapter_number, chapter_text in enumerate(chapters):\n",
      "    file_name = 'data/alice/chapter-'+str(chapter_number+1)+'.txt'\n",
      "    with open(file_name,'w') as f: \n",
      "        f.write(chapter_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Now using NTLK, let's create a corpus from those files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
      "\n",
      "\n",
      "corpusdir = 'data/alice/' # Directory of corpus.\n",
      "corpus0 = PlaintextCorpusReader(corpusdir, '.*')\n",
      "corpus  = nltk.Text(corpus0.words())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Analyzing text\n",
      "\n",
      "###How many words are there?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How many unique words are there?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(corpus))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How do we access particular words?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How do we find words in contexts?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus.concordance('Alice')  # you can also try corpus.concordance('Alice',lines=all) or lines = 100, etc."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How to find words in similar contexts (in a sentence)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus.similar('caterpillar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How to find the common contexts (within a sentence)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus.common_contexts(['hatter','queen'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Now let's build a frequency distribution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd = nltk.FreqDist(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How many times does a word occur?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd['Alice']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###What are all the words?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Which words occur only once? (hapax legomenon)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd.hapaxes()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How do you plot the most frequency words?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd.plot(20,cumulative=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How to normalize text?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_corpus = [w.lower() for w in corpus if w.isalpha()] # keep only alphabetic words and lowercase it"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "print stopwords\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_corpus = [w for w in new_corpus if not w in stopwords] # erase words in stoplist\n",
      "corpus = nltk.Text(new_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd2 = nltk.FreqDist(corpus) # create a FreqDist (Frequency Distribution)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd2.plot(20,cumulative=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How do you search for words?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus.dispersion_plot(['alice','rabbit','queen'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How do you find collocations (words that occur together)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus.collocations(num=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How to find bi-grams, tri-grams, and n-grams?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for x in nltk.bigrams(corpus):\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for x in nltk.trigrams(corpus):\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for x in nltk.ngrams(corpus,7):\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.pos_tag(corpus[0:1000])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The End!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}